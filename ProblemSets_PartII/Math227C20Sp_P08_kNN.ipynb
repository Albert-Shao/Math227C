{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "options(repr.plot.width=5, repr.plot.height=4)\n",
    "install.packages(\"ggplot2\")\n",
    "library(\"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 8: k-Nearest-Neighbors and the variance-bias tradeoff\n",
    "\n",
    "In polynomial regression, we use data to learn a model $y(x)$, (specifically, we learn the coefficients $\\beta$) so that we may predict what $y_i$ is generated by an $x_i$. \n",
    "\n",
    "One challenge is choosing the proper order of polynomial. Too low-order, and you will bias the model towards a simple explanation. Too high-order, and your model will vary for slightly different input data. \n",
    "\n",
    "In this problem set, we observe the same variance-bias tradeoff in a different method of learning a predictive model.\n",
    "\n",
    "## k-Nearest-Neighbors\n",
    "\n",
    "Neutrophils are white blood cells that activate to fight infection. Active neutrophils have slightly different size and deformability compared to inactive neutrophils.  \n",
    "\n",
    "Suppose you have build a high-throughput cell measurement tool that detects the size $x_1$ and deformability $x_2$ of cells. Suppose you have $N$ observations of $y, x_1, x_2$ where $y=1$ for active and $y=0$ for inactive cells. \n",
    "\n",
    "In the k-Nearest-Neighbors method, a new point $(x^n_1,x^n_2)$ has a probabilty of having $y^n=1$ given by\n",
    "$$P(y^n=1) = \\frac{1}{k} \\sum_{i\\in k \\mbox{nearest}} I(y^i=1)$$\n",
    "where $k \\mbox{nearest}$ are the k nearest neighbors and $I(y^i=1)=1$ is the indicator function that $y^i==1$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated data\n",
    "\n",
    "# This simulation code will make a sample of size 4.25*NPerQuadrant. \n",
    "# So, if NPerQuadrant=100, then the sample size is 425. \n",
    "\n",
    "NPerQuadrant <- 100\n",
    "\n",
    "x_1_mean1 <- +0.6\n",
    "x_1_mean2 <- -0.6\n",
    "x_1_mean3 <- +0.6\n",
    "x_1_mean4 <- -0.4\n",
    "x_1_mean5 <- +0.4\n",
    "x_2_mean1 <- +0.6\n",
    "x_2_mean2 <- +0.6\n",
    "x_2_mean3 <- -0.5\n",
    "x_2_mean4 <- -0.6\n",
    "x_2_mean5 <- +0.9\n",
    "\n",
    "x_1_std1 <- 0.5\n",
    "x_1_std2 <- 0.4\n",
    "x_1_std3 <- 0.5\n",
    "x_1_std4 <- 0.5\n",
    "x_1_std5 <- 0.1\n",
    "\n",
    "x_2_std1 <- 0.6\n",
    "x_2_std2 <- 0.3\n",
    "x_2_std3 <- 0.4\n",
    "x_2_std4 <- 0.4\n",
    "x_2_std5 <- 0.2\n",
    "\n",
    "\n",
    "sizes1 = rnorm(NPerQuadrant,x_1_mean1,x_1_std1)\n",
    "sizes2 = rnorm(NPerQuadrant,x_1_mean2,x_1_std2)\n",
    "sizes3 = rnorm(NPerQuadrant,x_1_mean3,x_1_std3)\n",
    "sizes4 = rnorm(NPerQuadrant,x_1_mean4,x_1_std4)\n",
    "sizes5 = rnorm(NPerQuadrant/4,x_1_mean5,x_1_std5)\n",
    "\n",
    "deformabilities1 = rnorm(NPerQuadrant,x_2_mean1,x_2_std1)\n",
    "deformabilities2 = rnorm(NPerQuadrant,x_2_mean2,x_2_std2)\n",
    "deformabilities3 = rnorm(NPerQuadrant,x_2_mean3,x_2_std3)\n",
    "deformabilities4 = rnorm(NPerQuadrant,x_2_mean4,x_2_std4)\n",
    "deformabilities5 = rnorm(NPerQuadrant/4,x_2_mean5,x_2_std5)\n",
    "\n",
    "sizes = c(sizes1,sizes2, sizes3,sizes4,sizes5)\n",
    "deformabilities = c(deformabilities1,deformabilities2, deformabilities3,deformabilities4,deformabilities5)\n",
    "\n",
    "activated = c(rep(1,NPerQuadrant), rep(1,NPerQuadrant), rep(1,NPerQuadrant), rep(0,NPerQuadrant), rep(0,NPerQuadrant/4))\n",
    "\n",
    "df <- data.frame(\"Activated\" = activated, \"Size\" = sizes, \"Deformability\" = deformabilities)\n",
    "\n",
    "# shuffle\n",
    "df <- df[sample(nrow(df)),]\n",
    "\n",
    "head(df)\n",
    "\n",
    "ggplot(df, aes(x=Size, y=Deformability, color=Activated)) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part, let's just choose a k value arbitrarily.\n",
    "\n",
    "library(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testValues = cbind(runif(2000,-2,2),runif(2000,-2,2))\n",
    "\n",
    "train.df <- df[,c(\"Size\",\"Deformability\")]\n",
    "train.factor <- df$Activated\n",
    "\n",
    "knn.predValues = knn(train.df, testValues, train.factor, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the test results\n",
    "dfTest = data.frame(\"Activated\" = knn.predValues, \"Size\" = testValues[,1], \"Deformability\" = testValues[,2])\n",
    "head(dfTest)\n",
    "ggplot(dfTest, aes(x=Size, y=Deformability, color=Activated)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-bias tradeoff in kNN\n",
    "\n",
    "This method has several advantages over, e.g., logistic regression. For example, it can more easily handle curved boundaries between the two cell types.\n",
    "\n",
    "However, the same challenge as with polynomial regression remains: How do you choose $k$? If you choose $k$ too small, you will have a very rough boundary, and the predictions will vary if you had a slightly different sample. If you choose $k$ too large, you will bias your predictions to a smoother boundary, missing details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one simulated data set of size 425 (as it is currently set to do), test the kNN classifier on a uniform sample in $x_1 \\in (-2.0,2.0)$ and $x_2 \\in (-2.0,2.0)$ (as in the above code blocks). \n",
    "\n",
    "## Part 1\n",
    "\n",
    "As our Goodness of Fit measure, we will use the __fraction of correct predictions__ (out of 425). Compute this for the above data set, with $k=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Still for a fixed $k$, compute the average goodness of fit for 100 repeated simulated samples, each of size 425. (Note we could not do this in real life.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Compute the average goodness-of-fit over 100 repeated samples, each of size 425, as above. \n",
    "\n",
    "First, you may do this by manually experimenting with different $k$ values. What $k$ value optimizes the goodness-of-fit?\n",
    "\n",
    "__(Out of class part)__ Next, perform a sweep over $k=1$ to $k=200$. Plot Goodness-of-fit versus $k$. What is the $k$ value that optimizes goodness-of-fit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "The function knn has a built-in feature do perform cross-validation. Read the knn function documentation. Perform cross-validation over a range of $k=1$ to $k=200$. Pick whatever you want for splitting the data into a training set and validation set.\n",
    "\n",
    "What value of $k$ optimized goodness-of-fit according to cross-validation?\n",
    "\n",
    "Does it agree with the value of $k$ found in Part 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
